# ML Regularization

#### üå± The project was created as practical research at the Peter the Great St. Petersburg State University.

## Abstract

This project explores the application of regularization as a method of combating overfitting in machine learning. 
Retraining is a situation where the model remembers training data too well and generalizes new data poorly. 
Regularization allows you to control the complexity of the model and prevent overfitting.

The project examines various types of regularization, such as $L_1$-, $L_2$-regularization and Dropout, 
as well as their impact on model quality and generalizing ability. 
Experiments are being conducted with Titanic (model: LogRegression) and MNIST (model: ConvNN) datasets to demonstrate their effectiveness.

`Key words:` machine learning, neural networks, logistic regression, convolutional neural network, $L_1$-, $L_2$-regularization, Dropout, PyTorh, Matplotlib, SciKit-Learn

–¢–µ–º–∞ –¥–æ–∫–ª–∞–¥–∞: –ú–µ—Ç–æ–¥—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏

–ß–ª–µ–Ω—ã –∫–æ–º–∞–Ω–¥—ã: –ë–µ–ª–æ—É—Å, –£—à–∫–∞—Ä—ë–≤
